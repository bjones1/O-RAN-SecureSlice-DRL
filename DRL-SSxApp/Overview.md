# DRL Secure Slicing agent training and inference

## **Objective**

The objective is to maximize the overall throughput for all slices by mapping resource blocks (RBs) while ensuring minimum QoS agreements are met and maximum thresholds for each slice are upheld via secure slicing:

$ \\max\_{T_1, T_2, T_3} \\sum\_{i=1}^{3} T_i $

where $ T_1 $, $ T_2 $, and $ T_3 $ represent the throughput for URLLC, eMBB,
and Medium slices, respectively.

## **Constraints**

The optimization problem is subject to the following constraints:

1\. **QoS Constraint for URLLC:** The throughput of URLLC must satisfy
theultra-reliable and low-latency requirement while being constrained by the
minimum QoS threshold for the Medium slice. This can be expressed as:

$ \\theta\_{\\text{min}} < \\text{QoS}\_{\\text{URLLC}}(T_1) < \\theta\_{\\text{URLLC}} $

where $ \\theta\_{\\text{URLLC}} $ is the minimum QoS threshold for the
URLLCslice and $ \\theta\_{\\text{min}} $ is the minimum QoS threshold for the
Medium slice.

2\. **QoS Constraint for eMBB:** The throughput of eMBB must meet the enhanced

broadband requirement:

$ \\text{QoS}\_{\\text{eMBB}}(T_2) \\geq \\theta\_{\\text{eMBB}} \\quad \\text{(high data rate requirement)} $

where $ \\theta\_{\\text{eMBB}} $ is the minimum QoS threshold for the eMBB slice.

3\. **QoS Constraint for Medium Slice:** The throughput of the Medium slice must satisfy its QoS requirements, which is between URLLC and eMBB:

$ \\theta\_{\\text{URLLC}} \\leq \\text{QoS}\_{\\text{Medium}}(T_3) \\leq \\theta\_{\\text{eMBB}} $

4\. **Resource Allocation Constraint:** The total allocated resources (Physical Resource Blocks) for all slices cannot exceed the available resources $ R\_{\\text{total}} $:$ \\sum\_{i=1}^{3} R_i \\leq R\_{\\text{total}} $

where $ R_i $ is the resource allocation for slice $ i $.

5\. **Secure Slicing Constraint:** All UEs in the slice must operate below the maximum threshold; otherwise, they are considered malicious and may compromise the resources of legitimate UEs. Let $ \\tau\_{\\text{max}} $ represent the maximum allowable resource usage per UE. The constraint can be expressed as:

$ S\_{u,i} < \\tau\_{\\text{max}}, \\quad \\forall u \\in U_i $

where $ S\_{u,i} $ is the resource usage for UE $ u $ in slice $ i $, and $ U_i $ is the set of UEs in slice $ i $.

## **Approach**

![](../documentation/images/drl-ss-xapp-1.png)

The diagram outlines a framework for allocating physical resource blocks (PRBs) among user equipment (UE) slices using a Deep Reinforcement Learning (DRL) agent in a secure slicing xApp environment.

### Key Components

 **DRL Agent**: Uses Key Performance Metrics (KPM) as input, actions for PRB reallocation, and network throughput as the reward. The agent's operations include:

**State**: Input from KPM.

**Action**: Adjust PRB allocations.

**Reward**: Based on network throughput performance.

**Slice Configurator**: Implements the DRL agentâ€™s actions by reallocating PRBs to slices.

**Secure Slicing xApp**: Executes resource adjustments and handles UE security.

**UE Instances**: Devices bound to slices, with the potential to be moved to a secure slice if detected as malicious.

**iperf3 Server**: Generates traffic in the Downlink for user traffic.

### DRL Agent Actions

1 - 3. **Increase PRBs**: Adds 5 PRBs if a slice's throughput is insufficient but below or equal to the SLA. Removes 5 PRB if throughput exceeds the SLA requirements and gives 0 reward for that episode. 

4. **Secure UE**: Moves UEs to a secure slice if malicious behavior is detected.

### Workflow

1\. The DRL agent receives real-time KPM data and adjusts PRBs through the Slice Configurator.

2\. The iperf3 server provides throughput feedback to guide resource allocation.

3\. A reward function evaluates actions to maximize UE throughput while ensuring fairness and preventing malicious activity.

### Security & Fairness

**Proportional Fairness**: Ensures balanced PRB allocation across slices.

**Secure Slicing**: Isolates malicious UEs, allocating no PRBs to them, to protect network integrity.

**srsRAN Scheduling Breakdown and System Model**

![](../documentation/images/ssxapp.png)

In srsRAN 4G, the configuration provided maps resource blocks (RBs) based on several key parameters. With SISO (Single Input, Single Output) and transmission mode (TM1), the system allocates RBs across the physical layer considering bandwidth (bw=10), which corresponds to a 10 MHz system, and the subframe configuration. The scheduler assigns RBs dynamically based on user demands, channel conditions, and available bandwidth. 

The MCS (modulation and coding scheme) is set to automatic, allowing the system to adaptively select the most efficient scheme. PDSCH and PUSCH are then mapped to these allocated RBs for downlink and uplink transmissions, optimizing the use of the available frequency-time resources. We have 50 RBs in 10 MHz and 100 in 20 Mhz. We have alot more than this though since they are virtualized for the purpose of the DRL-SSxApp.

 https://github.com/openaicellular/srsRAN-e2/blob/master/srsenb/enb.conf.example

## Results

DQN:

**Hyperparameters and environment-specific constants**

LR = 1e-4\\

BATCH_SIZE = 32\\

BUFFER_SIZE = int(1e5)\\

UPDATE_EVERY = 4\\

TAU = 5e-4

action_prbs = \[2897, 965, 91\] if episode % 400 == 0: # Decay epsilon every

400 episodes

final average reward 26444471

![image](../documentation/images/DQN.png)

DDQN:

**Hyperparameters and environment-specific constants**

LR = 1e-4\\

BATCH_SIZE = 32\\

BUFFER_SIZE = int(1e5)\\

UPDATE_EVERY = 4\\

TAU = 5e-4

action_prbs = \[2897, 965, 91\] if episode % 400 == 0: # Decay epsilon every

400 episodes

final average reward 26043123

![image](../documentation/images/DDQN.png)

Dueling DQN

**Hyperparameters and environment-specific constants**

LR = 1e-4\\

BATCH_SIZE = 32\\

BUFFER_SIZE = int(1e5)\\

UPDATE_EVERY = 4\\

TAU = 5e-4

action_prbs = \[2897, 965, 91\] if episode % 400 == 0: 

Decay epsilon every 100 episodes

final average reward 25155529

![Dueling DQN](../documentation/images/duelingDQN.png)